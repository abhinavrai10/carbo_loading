{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6106b13f-49f1-48f8-8306-c15c60b091fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze = spark.sql(''' describe external location `bronze` ''').select('url').collect()[0][0]\n",
    "silver = spark.sql(''' describe external location `silver` ''').select('url').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c303a8-9583-40ce-bc17-762035c30a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, trim, lower, length, lpad, sum as pyspark_sum\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee424a7-f0cf-4feb-8349-3155060604ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"PARQUET\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .load(f\"{bronze}/dh_causal_lookup\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a77aa4e-b1cc-456f-9e2c-aee831d6606f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0355cc2-04d4-42dd-891d-3f734b290227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    print(f\"df before removing duplicates: {df.count()}\")\n",
    "    df_non_duplicates = df.dropDuplicates([\"upc\", \"store\", \"week\"])\n",
    "    return df_non_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb09c0c5-98c2-433a-a7ad-6b2407d2cf36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20d846c-884a-40ef-89a5-d9d83b1226d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    df_missing = df.fillna({\"feature_desc\": \"Not Specified\", \"display_desc\": \"Not Specified\"})\n",
    "    return df_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1f6679-d714-4044-8c3d-695820162818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Standardize Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874b0a6a-e8c1-4a34-8453-1bb3cc50d448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def standardize_formats(df):\n",
    "    df_standardized = df.withColumn(\"feature_desc\", trim(lower(col(\"feature_desc\")))) \\\n",
    "             .withColumn(\"display_desc\", trim(lower(col(\"display_desc\"))))\n",
    "    return df_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21c1088-4e24-437d-8a28-94ba154ddd79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Address Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0546c2-82cc-4f69-8bbc-7603b2aeaf11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_outliers(df, week_range=(1, 104), geography_range=(1, 2)):\n",
    "    print(f\"df before outliers check: {df.count()}\")\n",
    "    df_outliers_check = df.filter(col(\"week\").between(week_range[0], week_range[1]) & \n",
    "                     col(\"geography\").between(geography_range[0], geography_range[1]))\n",
    "    return df_outliers_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e4d4fe-6eed-4280-90d1-6d3773b1e4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Validate Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c7dd749-e9ee-4ff8-8c85-8e4e6bd2d279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def enforce_data_types(df):\n",
    "    df_types = df.withColumn(\"upc\", lpad(col(\"upc\").cast(StringType()), 10, \"0\")) \\\n",
    "             .withColumn(\"store\", col(\"store\").cast(IntegerType())) \\\n",
    "             .withColumn(\"week\", col(\"week\").cast(IntegerType())) \\\n",
    "             .withColumn(\"geography\", col(\"geography\").cast(IntegerType())) \\\n",
    "             .withColumn(\"feature_desc\", col(\"feature_desc\").cast(StringType())) \\\n",
    "             .withColumn(\"display_desc\", col(\"display_desc\").cast(StringType()))\n",
    "    return df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e87e92-14a8-45b2-8c0b-eb72e3773456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main(df, delta_path=f\"{silver}/dh_causal_lookup\"):\n",
    "    print(\"Starting dh_causal_lookup cleaning process...\")\n",
    "    print(\"Initial row count:\", df.count())\n",
    "    display(df.limit(5))\n",
    "    \n",
    "    # Step 1\n",
    "    df_step1 = handle_missing_values(df)\n",
    "    print(\"\\nStep 1 - Handle Missing Values\")\n",
    "    null_counts = df_step1.select([col(c).isNull().cast(\"int\").alias(c) for c in df_step1.columns]).agg(\n",
    "        *[pyspark_sum(col(c)).alias(f\"{c}_null_count\") for c in df_step1.columns]\n",
    "    ).collect()[0].asDict()\n",
    "    print(f\"Null counts after handling: {null_counts}\")\n",
    "    display(df_step1.limit(5))\n",
    "    \n",
    "    # Step 2\n",
    "    df_step2 = remove_duplicates(df_step1)\n",
    "    print(\"\\nStep 2 - Remove Duplicates\")\n",
    "    initial_count = df_step1.count()\n",
    "    final_count = df_step2.count()\n",
    "    print(f\"Initial row count: {initial_count}, Final row count: {final_count}, Duplicates removed: {initial_count - final_count}\")\n",
    "    display(df_step2.limit(5))\n",
    "    \n",
    "    # Step 3\n",
    "    df_step3 = standardize_formats(df_step2)\n",
    "    print(\"\\nStep 3 - Standardize Formats\")\n",
    "    print(\"Distinct feature_desc values:\")\n",
    "    display(df_step3.select(\"feature_desc\").distinct().limit(5))\n",
    "    print(\"Distinct display_desc values:\")\n",
    "    display(df_step3.select(\"display_desc\").distinct().limit(5))\n",
    "    display(df_step3.limit(5))\n",
    "    \n",
    "    # Step 4\n",
    "    df_step4 = filter_outliers(df_step3)\n",
    "    print(\"\\nStep 4 - Address Outliers\")\n",
    "    initial_count = df_step3.count()\n",
    "    final_count = df_step4.count()\n",
    "    week_range = (1, 104)\n",
    "    geography_range = (1, 2)\n",
    "    print(f\"Rows before filtering: {initial_count}, Rows after filtering: {final_count}, Outliers removed: {initial_count - final_count}\")\n",
    "    print(f\"Week range checked: {week_range}, Geography range checked: {geography_range}\")\n",
    "    display(df_step4.limit(5))\n",
    "    \n",
    "    # Step 5\n",
    "    df_cleaned = enforce_data_types(df_step4)\n",
    "    print(\"\\nStep 5 - Validate Data Types\")\n",
    "    invalid_upc_count = df_cleaned.filter(length(col(\"upc\")) != 10).count()\n",
    "    print(f\"UPCs with incorrect length: {invalid_upc_count}\")\n",
    "    print(\"Schema after type enforcement:\")\n",
    "    df_cleaned.printSchema()\n",
    "    display(df_cleaned.limit(5))\n",
    "    \n",
    "    print(\"\\nCleaning process completed!\")\n",
    "    print(f\"Final row count: {df_cleaned.count()}\")\n",
    "    display(df_cleaned.limit(5))\n",
    "    # Delta Lake Merge\n",
    "    if DeltaTable.isDeltaTable(spark, delta_path):\n",
    "        print(\"\\nDelta table exists, performing MERGE...\")\n",
    "        delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "        delta_table.alias(\"target\")\\\n",
    "            .merge(\n",
    "                df_cleaned.alias(\"source\"),\n",
    "                \"target.upc = source.upc AND target.store = source.store AND target.week = source.week\"\n",
    "            )\\\n",
    "            .whenMatchedUpdateAll()\\\n",
    "            .whenNotMatchedInsertAll()\\\n",
    "            .execute()\n",
    "    else:\n",
    "        print(\"\\nDelta table does not exist, creating new table...\")\n",
    "        df_cleaned.write.format(\"delta\")\\\n",
    "                    .mode(\"overwrite\")\\\n",
    "                    .option(\"path\", delta_path)\\\n",
    "                    .saveAsTable(\"carbo_catalog.silver.dh_causal_lookup\")\n",
    "    \n",
    "    # Load and log final state\n",
    "    final_df = spark.read.format(\"delta\").load(delta_path)\n",
    "    print(\"\\nCleaning and merge completed!\")\n",
    "    print(f\"Final row count in Delta table: {final_df.count()}\")\n",
    "    display(final_df.limit(5))\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "df = main(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43d3f47-f254-41f9-9486-7f86c1f9e4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) FROM `carbo_catalog`.`silver`.`dh_causal_lookup`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6713428850707943,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_causal_lookup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
