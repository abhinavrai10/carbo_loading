{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2afd752-ae9f-4984-9208-6323617b1654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver = spark.sql(''' describe external location `silver` ''').select('url').collect()[0][0]\n",
    "gold = spark.sql(''' describe external location `gold` ''').select('url').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72379eed-57b9-4ca1-826d-5b94eae60c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number, sha2, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736c3905-74ee-44ce-ad83-c467127bbc96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_sales_fact():\n",
    "    silver_path = f\"DELTA.`{silver}dh_transactions`\"\n",
    "    gold_fact_path = f\"{gold}sales_fact\"\n",
    "    table_name = \"`carbo_catalog`.`gold`.sales_fact\"\n",
    "\n",
    "    print(\"Starting pipeline for sales_fact\")\n",
    "    try:\n",
    "        # Step 1: Read silver data\n",
    "        df_silver = spark.sql(f\"SELECT * FROM {silver_path}\")\n",
    "\n",
    "        # Step 2: Read dimension tables\n",
    "        df_time = spark.sql(\"SELECT * FROM `carbo_catalog`.`gold`.dim_time\")\n",
    "        df_product = spark.sql(\"SELECT * FROM `carbo_catalog`.`gold`.dim_product\")\n",
    "        df_household = spark.sql(\"SELECT * FROM `carbo_catalog`.`gold`.dim_household\")\n",
    "        df_store = spark.sql(\"SELECT * FROM `carbo_catalog`.`gold`.dim_store\")\n",
    "        df_trade = spark.sql(\"SELECT * FROM `carbo_catalog`.`gold`.dim_trade\")\n",
    "\n",
    "        # Step 3: Join silver data with dimension tables\n",
    "        df_fact = df_silver.join(\n",
    "            df_time,\n",
    "            (df_silver.day == df_time.day) & \n",
    "            (df_silver.week == df_time.week) & \n",
    "            (df_silver.time_of_transaction == df_time.time_of_transaction),\n",
    "            \"left\"\n",
    "        ).join(\n",
    "            df_product,\n",
    "            df_silver.upc == df_product.upc,\n",
    "            \"left\"\n",
    "        ).join(\n",
    "            df_household,\n",
    "            df_silver.household == df_household.household,\n",
    "            \"left\"\n",
    "        ).join(\n",
    "            df_store,\n",
    "            df_silver.store == df_store.store,\n",
    "            \"left\"\n",
    "        ).join(\n",
    "            df_trade,\n",
    "            (df_silver.upc == df_trade.upc) & \n",
    "            (df_silver.store == df_trade.store) & \n",
    "            (df_silver.week == df_trade.week),\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            df_silver.day,\n",
    "            df_silver.time_of_transaction,\n",
    "            df_silver.store,\n",
    "            df_silver.household,\n",
    "            df_silver.basket,\n",
    "            df_silver.upc,\n",
    "            df_silver.dollar_sales,\n",
    "            df_silver.units,\n",
    "            df_silver.coupon,\n",
    "            df_time.time_key,\n",
    "            df_product.product_key,\n",
    "            df_household.household_key,\n",
    "            df_store.store_key,\n",
    "            df_trade.trade_key\n",
    "        )\n",
    "\n",
    "        # Step 4: Add transaction_key\n",
    "        df_fact = df_fact.withColumn(\n",
    "            \"transaction_key\",\n",
    "            sha2(concat_ws(\"-\", \"day\", \"time_of_transaction\", \"store\", \"household\", \"basket\", \"upc\"), 256)\n",
    "        )\n",
    "\n",
    "        # Step 5: Select final columns\n",
    "        df_fact = df_fact.select(\n",
    "            \"transaction_key\",\n",
    "            \"time_key\",\n",
    "            \"product_key\",\n",
    "            \"household_key\",\n",
    "            \"store_key\",\n",
    "            \"trade_key\",\n",
    "            \"dollar_sales\",\n",
    "            \"units\",\n",
    "            \"coupon\",\n",
    "            \"basket\"\n",
    "        )\n",
    "\n",
    "        # Step 6: Write to Delta Lake with incremental load\n",
    "        if df_fact.count() == 0:\n",
    "            print(\"No data to process for sales_fact\")\n",
    "            return\n",
    "\n",
    "        if spark.catalog.tableExists(table_name):\n",
    "            print(\"Performing incremental load for sales_fact\")\n",
    "            delta_table = DeltaTable.forPath(spark, gold_fact_path)\n",
    "            delta_table.alias(\"trg\").merge(\n",
    "                df_fact.alias(\"src\"),\n",
    "                \"trg.transaction_key = src.transaction_key\"\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "            print(\"Incremental update completed for sales_fact\")\n",
    "        else:\n",
    "            print(\"Performing initial load for sales_fact\")\n",
    "            df_fact.write.format(\"delta\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .option(\"path\", gold_fact_path)\\\n",
    "                .saveAsTable(table_name)\n",
    "            print(\"Initial load completed for sales_fact\")\n",
    "\n",
    "        display(df_fact)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed for sales_fact: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"Pipeline completed for sales_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f249786-dcaf-4a63-a935-abe75ee1c698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execute the function\n",
    "create_sales_fact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75ce3807-cbe1-46a3-9519-1f1ad6983500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT count(*) FROM `carbo_catalog`.`gold`.sales_fact\")\n",
    "# df = spark.sql(\"SELECT count(*) FROM `carbo_catalog`.`silver`.dh_transactions\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c768bd5f-9ad1-48f1-b6ab-44d7ef839c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.sql(f\"SELECT * FROM DELTA.`{silver}dh_transactions`\")\n",
    "display(df_silver)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_fact_carbo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
